{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultiLayeredNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRX6mFtyBjua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6588034f-3a51-434d-b66b-ebed375239bd"
      },
      "source": [
        "#Reading data from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viyl1JfwC8p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SEED = 2017"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31zD03vGC8uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"gdrive/My Drive/DeepLearningCodeBook/winequality-red.csv\",  sep=';');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClZpambZC8zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data['quality']\n",
        "X = data.drop(['quality'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVzChYA6C81B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "7d18a496-f9de-4dcf-ccad-7320b87ae372"
      },
      "source": [
        "X"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.47</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.067</td>\n",
              "      <td>18.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.99549</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  ...    pH  sulphates  alcohol\n",
              "0               7.4             0.700         0.00  ...  3.51       0.56      9.4\n",
              "1               7.8             0.880         0.00  ...  3.20       0.68      9.8\n",
              "2               7.8             0.760         0.04  ...  3.26       0.65      9.8\n",
              "3              11.2             0.280         0.56  ...  3.16       0.58      9.8\n",
              "4               7.4             0.700         0.00  ...  3.51       0.56      9.4\n",
              "...             ...               ...          ...  ...   ...        ...      ...\n",
              "1594            6.2             0.600         0.08  ...  3.45       0.58     10.5\n",
              "1595            5.9             0.550         0.10  ...  3.52       0.76     11.2\n",
              "1596            6.3             0.510         0.13  ...  3.42       0.75     11.0\n",
              "1597            5.9             0.645         0.12  ...  3.57       0.71     10.2\n",
              "1598            6.0             0.310         0.47  ...  3.39       0.66     11.0\n",
              "\n",
              "[1599 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM3rbQu2C85B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size = 0.2 , random_state = SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4TemqHpC8ti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "9c1c7f43-22b3-4303-a9ef-43a5562c28fd"
      },
      "source": [
        "print('average quality of training set : {:.4f}'.format(y_train.mean()))\n",
        "X_train.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average quality of training set : 5.6231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>7.3</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.30</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.080</td>\n",
              "      <td>33.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0.99690</td>\n",
              "      <td>3.41</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>920</th>\n",
              "      <td>9.6</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.37</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.091</td>\n",
              "      <td>10.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.99786</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.56</td>\n",
              "      <td>10.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>7.7</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.26</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.052</td>\n",
              "      <td>19.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.99510</td>\n",
              "      <td>3.15</td>\n",
              "      <td>0.79</td>\n",
              "      <td>10.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>10.5</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.47</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.066</td>\n",
              "      <td>6.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.15</td>\n",
              "      <td>0.90</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>13.2</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.52</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.071</td>\n",
              "      <td>12.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.00060</td>\n",
              "      <td>3.10</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  ...    pH  sulphates  alcohol\n",
              "1140            7.3              0.40         0.30  ...  3.41       0.65      9.5\n",
              "920             9.6              0.41         0.37  ...  3.24       0.56     10.5\n",
              "1198            7.7              0.26         0.26  ...  3.15       0.79     10.9\n",
              "423            10.5              0.24         0.47  ...  3.15       0.90     11.0\n",
              "601            13.2              0.46         0.52  ...  3.10       0.56      9.0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QfeJHoKC8oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train =pd.DataFrame(scaler.transform(X_train))\n",
        "X_test = pd.DataFrame(scaler.transform(X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doVJUhMMJMrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(200, input_dim = X_train.shape[1] , activation=\"relu\"))\n",
        "model.add(Dense(25, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "opt = Adam()\n",
        "\n",
        "model.compile(loss = 'mse' , optimizer = opt , metrics = ['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlwJoRiUJMub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# study these at\n",
        "# https://stackoverflow.com/questions/43906048/keras-early-stopping\n",
        "\n",
        "callbacks = [\n",
        "             EarlyStopping(monitor='val_acc', patience=200, verbose=2),\n",
        "             ModelCheckpoint('gdrive/My Drive/DeepLearningCodeBook/multi_layer_best_model.h5', monitor='val_acc', save_best_only=True, verbose=0)\n",
        "            ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPZBaPe4JMox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "n_epochs = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLaLCFWQM8GZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a215f5e-5333-4d02-c21a-af27fa3d74df"
      },
      "source": [
        "model.fit(X_train.values , y_train , batch_size=batch_size , epochs = n_epochs , validation_split=0.2,     \n",
        "             verbose=2,\n",
        "             validation_data=(X_test.values, y_test),\n",
        "             callbacks=callbacks)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1279 samples, validate on 320 samples\n",
            "Epoch 1/5000\n",
            " - 0s - loss: 0.2669 - acc: 0.7099 - val_loss: 0.3607 - val_acc: 0.6406\n",
            "Epoch 2/5000\n",
            " - 0s - loss: 0.2640 - acc: 0.7076 - val_loss: 0.3836 - val_acc: 0.6438\n",
            "Epoch 3/5000\n",
            " - 0s - loss: 0.2691 - acc: 0.6794 - val_loss: 0.3805 - val_acc: 0.6062\n",
            "Epoch 4/5000\n",
            " - 0s - loss: 0.2710 - acc: 0.6959 - val_loss: 0.3857 - val_acc: 0.6094\n",
            "Epoch 5/5000\n",
            " - 0s - loss: 0.2650 - acc: 0.7037 - val_loss: 0.3696 - val_acc: 0.6687\n",
            "Epoch 6/5000\n",
            " - 0s - loss: 0.2635 - acc: 0.6935 - val_loss: 0.3637 - val_acc: 0.6312\n",
            "Epoch 7/5000\n",
            " - 0s - loss: 0.2635 - acc: 0.7005 - val_loss: 0.3566 - val_acc: 0.6469\n",
            "Epoch 8/5000\n",
            " - 0s - loss: 0.2615 - acc: 0.6982 - val_loss: 0.3649 - val_acc: 0.6406\n",
            "Epoch 9/5000\n",
            " - 0s - loss: 0.2628 - acc: 0.7084 - val_loss: 0.3710 - val_acc: 0.6562\n",
            "Epoch 10/5000\n",
            " - 0s - loss: 0.2706 - acc: 0.7037 - val_loss: 0.3914 - val_acc: 0.6031\n",
            "Epoch 11/5000\n",
            " - 0s - loss: 0.2556 - acc: 0.7091 - val_loss: 0.3582 - val_acc: 0.6344\n",
            "Epoch 12/5000\n",
            " - 0s - loss: 0.2534 - acc: 0.7123 - val_loss: 0.3628 - val_acc: 0.6375\n",
            "Epoch 13/5000\n",
            " - 0s - loss: 0.2537 - acc: 0.7045 - val_loss: 0.3685 - val_acc: 0.6594\n",
            "Epoch 14/5000\n",
            " - 0s - loss: 0.2590 - acc: 0.7209 - val_loss: 0.3642 - val_acc: 0.6500\n",
            "Epoch 15/5000\n",
            " - 0s - loss: 0.2580 - acc: 0.7201 - val_loss: 0.3580 - val_acc: 0.6406\n",
            "Epoch 16/5000\n",
            " - 0s - loss: 0.2511 - acc: 0.7146 - val_loss: 0.3614 - val_acc: 0.6281\n",
            "Epoch 17/5000\n",
            " - 0s - loss: 0.2442 - acc: 0.7193 - val_loss: 0.3501 - val_acc: 0.6625\n",
            "Epoch 18/5000\n",
            " - 0s - loss: 0.2470 - acc: 0.7099 - val_loss: 0.3803 - val_acc: 0.6344\n",
            "Epoch 19/5000\n",
            " - 0s - loss: 0.2530 - acc: 0.7107 - val_loss: 0.3612 - val_acc: 0.6469\n",
            "Epoch 20/5000\n",
            " - 0s - loss: 0.2459 - acc: 0.7154 - val_loss: 0.3584 - val_acc: 0.6656\n",
            "Epoch 21/5000\n",
            " - 0s - loss: 0.2445 - acc: 0.7193 - val_loss: 0.3765 - val_acc: 0.6188\n",
            "Epoch 22/5000\n",
            " - 0s - loss: 0.2483 - acc: 0.7170 - val_loss: 0.3544 - val_acc: 0.6562\n",
            "Epoch 23/5000\n",
            " - 0s - loss: 0.2433 - acc: 0.7185 - val_loss: 0.3595 - val_acc: 0.6594\n",
            "Epoch 24/5000\n",
            " - 0s - loss: 0.2425 - acc: 0.7357 - val_loss: 0.3651 - val_acc: 0.6469\n",
            "Epoch 25/5000\n",
            " - 0s - loss: 0.2378 - acc: 0.7263 - val_loss: 0.3544 - val_acc: 0.6531\n",
            "Epoch 26/5000\n",
            " - 0s - loss: 0.2426 - acc: 0.7303 - val_loss: 0.3598 - val_acc: 0.6312\n",
            "Epoch 27/5000\n",
            " - 0s - loss: 0.2398 - acc: 0.7146 - val_loss: 0.3569 - val_acc: 0.6438\n",
            "Epoch 28/5000\n",
            " - 0s - loss: 0.2384 - acc: 0.7091 - val_loss: 0.3619 - val_acc: 0.6500\n",
            "Epoch 29/5000\n",
            " - 0s - loss: 0.2353 - acc: 0.7326 - val_loss: 0.3765 - val_acc: 0.6312\n",
            "Epoch 30/5000\n",
            " - 0s - loss: 0.2370 - acc: 0.7248 - val_loss: 0.3663 - val_acc: 0.6344\n",
            "Epoch 31/5000\n",
            " - 0s - loss: 0.2401 - acc: 0.7263 - val_loss: 0.3699 - val_acc: 0.6687\n",
            "Epoch 32/5000\n",
            " - 0s - loss: 0.2509 - acc: 0.7107 - val_loss: 0.3646 - val_acc: 0.6188\n",
            "Epoch 33/5000\n",
            " - 0s - loss: 0.2366 - acc: 0.7365 - val_loss: 0.3627 - val_acc: 0.6438\n",
            "Epoch 34/5000\n",
            " - 0s - loss: 0.2409 - acc: 0.7232 - val_loss: 0.3535 - val_acc: 0.6625\n",
            "Epoch 35/5000\n",
            " - 0s - loss: 0.2330 - acc: 0.7349 - val_loss: 0.3530 - val_acc: 0.6219\n",
            "Epoch 36/5000\n",
            " - 0s - loss: 0.2303 - acc: 0.7381 - val_loss: 0.3629 - val_acc: 0.6469\n",
            "Epoch 37/5000\n",
            " - 0s - loss: 0.2297 - acc: 0.7303 - val_loss: 0.3976 - val_acc: 0.6375\n",
            "Epoch 38/5000\n",
            " - 0s - loss: 0.2345 - acc: 0.7263 - val_loss: 0.3561 - val_acc: 0.6531\n",
            "Epoch 39/5000\n",
            " - 0s - loss: 0.2271 - acc: 0.7482 - val_loss: 0.3620 - val_acc: 0.6375\n",
            "Epoch 40/5000\n",
            " - 0s - loss: 0.2326 - acc: 0.7443 - val_loss: 0.3542 - val_acc: 0.6750\n",
            "Epoch 41/5000\n",
            " - 0s - loss: 0.2337 - acc: 0.7349 - val_loss: 0.3766 - val_acc: 0.6375\n",
            "Epoch 42/5000\n",
            " - 0s - loss: 0.2253 - acc: 0.7404 - val_loss: 0.3842 - val_acc: 0.6094\n",
            "Epoch 43/5000\n",
            " - 0s - loss: 0.2441 - acc: 0.7224 - val_loss: 0.3715 - val_acc: 0.6687\n",
            "Epoch 44/5000\n",
            " - 0s - loss: 0.2291 - acc: 0.7475 - val_loss: 0.3672 - val_acc: 0.6406\n",
            "Epoch 45/5000\n",
            " - 0s - loss: 0.2427 - acc: 0.7193 - val_loss: 0.3616 - val_acc: 0.6687\n",
            "Epoch 46/5000\n",
            " - 0s - loss: 0.2340 - acc: 0.7365 - val_loss: 0.3868 - val_acc: 0.6250\n",
            "Epoch 47/5000\n",
            " - 0s - loss: 0.2381 - acc: 0.7310 - val_loss: 0.3661 - val_acc: 0.6500\n",
            "Epoch 48/5000\n",
            " - 0s - loss: 0.2292 - acc: 0.7357 - val_loss: 0.3567 - val_acc: 0.6813\n",
            "Epoch 49/5000\n",
            " - 0s - loss: 0.2288 - acc: 0.7349 - val_loss: 0.3725 - val_acc: 0.6344\n",
            "Epoch 50/5000\n",
            " - 0s - loss: 0.2236 - acc: 0.7451 - val_loss: 0.3767 - val_acc: 0.6500\n",
            "Epoch 51/5000\n",
            " - 0s - loss: 0.2194 - acc: 0.7522 - val_loss: 0.3747 - val_acc: 0.6312\n",
            "Epoch 52/5000\n",
            " - 0s - loss: 0.2233 - acc: 0.7490 - val_loss: 0.3858 - val_acc: 0.6375\n",
            "Epoch 53/5000\n",
            " - 0s - loss: 0.2202 - acc: 0.7506 - val_loss: 0.3801 - val_acc: 0.6062\n",
            "Epoch 54/5000\n",
            " - 0s - loss: 0.2231 - acc: 0.7482 - val_loss: 0.3723 - val_acc: 0.6375\n",
            "Epoch 55/5000\n",
            " - 0s - loss: 0.2157 - acc: 0.7506 - val_loss: 0.3626 - val_acc: 0.6875\n",
            "Epoch 56/5000\n",
            " - 0s - loss: 0.2234 - acc: 0.7389 - val_loss: 0.3812 - val_acc: 0.6250\n",
            "Epoch 57/5000\n",
            " - 0s - loss: 0.2271 - acc: 0.7443 - val_loss: 0.3874 - val_acc: 0.6094\n",
            "Epoch 58/5000\n",
            " - 0s - loss: 0.2172 - acc: 0.7451 - val_loss: 0.3705 - val_acc: 0.6687\n",
            "Epoch 59/5000\n",
            " - 0s - loss: 0.2103 - acc: 0.7584 - val_loss: 0.3727 - val_acc: 0.6219\n",
            "Epoch 60/5000\n",
            " - 0s - loss: 0.2079 - acc: 0.7600 - val_loss: 0.3712 - val_acc: 0.6312\n",
            "Epoch 61/5000\n",
            " - 0s - loss: 0.2128 - acc: 0.7568 - val_loss: 0.3583 - val_acc: 0.6625\n",
            "Epoch 62/5000\n",
            " - 0s - loss: 0.2093 - acc: 0.7561 - val_loss: 0.3727 - val_acc: 0.6281\n",
            "Epoch 63/5000\n",
            " - 0s - loss: 0.2101 - acc: 0.7631 - val_loss: 0.3496 - val_acc: 0.6562\n",
            "Epoch 64/5000\n",
            " - 0s - loss: 0.2038 - acc: 0.7522 - val_loss: 0.3586 - val_acc: 0.6531\n",
            "Epoch 65/5000\n",
            " - 0s - loss: 0.2063 - acc: 0.7670 - val_loss: 0.3913 - val_acc: 0.6219\n",
            "Epoch 66/5000\n",
            " - 0s - loss: 0.2143 - acc: 0.7396 - val_loss: 0.3778 - val_acc: 0.6750\n",
            "Epoch 67/5000\n",
            " - 0s - loss: 0.2139 - acc: 0.7553 - val_loss: 0.3716 - val_acc: 0.6188\n",
            "Epoch 68/5000\n",
            " - 0s - loss: 0.1978 - acc: 0.7811 - val_loss: 0.3768 - val_acc: 0.6344\n",
            "Epoch 69/5000\n",
            " - 0s - loss: 0.1996 - acc: 0.7772 - val_loss: 0.3653 - val_acc: 0.6469\n",
            "Epoch 70/5000\n",
            " - 0s - loss: 0.2089 - acc: 0.7608 - val_loss: 0.3508 - val_acc: 0.6406\n",
            "Epoch 71/5000\n",
            " - 0s - loss: 0.2150 - acc: 0.7631 - val_loss: 0.3604 - val_acc: 0.6719\n",
            "Epoch 72/5000\n",
            " - 0s - loss: 0.2070 - acc: 0.7740 - val_loss: 0.3678 - val_acc: 0.6375\n",
            "Epoch 73/5000\n",
            " - 0s - loss: 0.2112 - acc: 0.7451 - val_loss: 0.3619 - val_acc: 0.6500\n",
            "Epoch 74/5000\n",
            " - 0s - loss: 0.2088 - acc: 0.7615 - val_loss: 0.3866 - val_acc: 0.6312\n",
            "Epoch 75/5000\n",
            " - 0s - loss: 0.2174 - acc: 0.7506 - val_loss: 0.3824 - val_acc: 0.6375\n",
            "Epoch 76/5000\n",
            " - 0s - loss: 0.1967 - acc: 0.7819 - val_loss: 0.3648 - val_acc: 0.6344\n",
            "Epoch 77/5000\n",
            " - 0s - loss: 0.1955 - acc: 0.7701 - val_loss: 0.3640 - val_acc: 0.6375\n",
            "Epoch 78/5000\n",
            " - 0s - loss: 0.1942 - acc: 0.7748 - val_loss: 0.3677 - val_acc: 0.6469\n",
            "Epoch 79/5000\n",
            " - 0s - loss: 0.1984 - acc: 0.7748 - val_loss: 0.3698 - val_acc: 0.6687\n",
            "Epoch 80/5000\n",
            " - 0s - loss: 0.1988 - acc: 0.7733 - val_loss: 0.3617 - val_acc: 0.6344\n",
            "Epoch 81/5000\n",
            " - 0s - loss: 0.1916 - acc: 0.7709 - val_loss: 0.3719 - val_acc: 0.6312\n",
            "Epoch 82/5000\n",
            " - 0s - loss: 0.1917 - acc: 0.7873 - val_loss: 0.3748 - val_acc: 0.6656\n",
            "Epoch 83/5000\n",
            " - 0s - loss: 0.1895 - acc: 0.7928 - val_loss: 0.3660 - val_acc: 0.6438\n",
            "Epoch 84/5000\n",
            " - 0s - loss: 0.1958 - acc: 0.7748 - val_loss: 0.3687 - val_acc: 0.6156\n",
            "Epoch 85/5000\n",
            " - 0s - loss: 0.1889 - acc: 0.7905 - val_loss: 0.3646 - val_acc: 0.6500\n",
            "Epoch 86/5000\n",
            " - 0s - loss: 0.1924 - acc: 0.7740 - val_loss: 0.3771 - val_acc: 0.6875\n",
            "Epoch 87/5000\n",
            " - 0s - loss: 0.1905 - acc: 0.7967 - val_loss: 0.3842 - val_acc: 0.6625\n",
            "Epoch 88/5000\n",
            " - 0s - loss: 0.1892 - acc: 0.7834 - val_loss: 0.4093 - val_acc: 0.6312\n",
            "Epoch 89/5000\n",
            " - 0s - loss: 0.1874 - acc: 0.7803 - val_loss: 0.3748 - val_acc: 0.6406\n",
            "Epoch 90/5000\n",
            " - 0s - loss: 0.1872 - acc: 0.7850 - val_loss: 0.3682 - val_acc: 0.6562\n",
            "Epoch 91/5000\n",
            " - 0s - loss: 0.1871 - acc: 0.7756 - val_loss: 0.3950 - val_acc: 0.6750\n",
            "Epoch 92/5000\n",
            " - 0s - loss: 0.2033 - acc: 0.7529 - val_loss: 0.3881 - val_acc: 0.6188\n",
            "Epoch 93/5000\n",
            " - 0s - loss: 0.1952 - acc: 0.7858 - val_loss: 0.3772 - val_acc: 0.6438\n",
            "Epoch 94/5000\n",
            " - 0s - loss: 0.1875 - acc: 0.7858 - val_loss: 0.3712 - val_acc: 0.6562\n",
            "Epoch 95/5000\n",
            " - 0s - loss: 0.1875 - acc: 0.7858 - val_loss: 0.3727 - val_acc: 0.6625\n",
            "Epoch 96/5000\n",
            " - 0s - loss: 0.1908 - acc: 0.7850 - val_loss: 0.3666 - val_acc: 0.6469\n",
            "Epoch 97/5000\n",
            " - 0s - loss: 0.1817 - acc: 0.7850 - val_loss: 0.3911 - val_acc: 0.6188\n",
            "Epoch 98/5000\n",
            " - 0s - loss: 0.1769 - acc: 0.7928 - val_loss: 0.3680 - val_acc: 0.6469\n",
            "Epoch 99/5000\n",
            " - 0s - loss: 0.1772 - acc: 0.7959 - val_loss: 0.3771 - val_acc: 0.7000\n",
            "Epoch 100/5000\n",
            " - 0s - loss: 0.1899 - acc: 0.7764 - val_loss: 0.3722 - val_acc: 0.6438\n",
            "Epoch 101/5000\n",
            " - 0s - loss: 0.1802 - acc: 0.7959 - val_loss: 0.4055 - val_acc: 0.6438\n",
            "Epoch 102/5000\n",
            " - 0s - loss: 0.1869 - acc: 0.7795 - val_loss: 0.3904 - val_acc: 0.6406\n",
            "Epoch 103/5000\n",
            " - 0s - loss: 0.1752 - acc: 0.8030 - val_loss: 0.3696 - val_acc: 0.6687\n",
            "Epoch 104/5000\n",
            " - 0s - loss: 0.1692 - acc: 0.8069 - val_loss: 0.3996 - val_acc: 0.6312\n",
            "Epoch 105/5000\n",
            " - 0s - loss: 0.1776 - acc: 0.7959 - val_loss: 0.3823 - val_acc: 0.6656\n",
            "Epoch 106/5000\n",
            " - 0s - loss: 0.1716 - acc: 0.8108 - val_loss: 0.3807 - val_acc: 0.6719\n",
            "Epoch 107/5000\n",
            " - 0s - loss: 0.1738 - acc: 0.8045 - val_loss: 0.3710 - val_acc: 0.6469\n",
            "Epoch 108/5000\n",
            " - 0s - loss: 0.1686 - acc: 0.8084 - val_loss: 0.3868 - val_acc: 0.6625\n",
            "Epoch 109/5000\n",
            " - 0s - loss: 0.1708 - acc: 0.8006 - val_loss: 0.3819 - val_acc: 0.6562\n",
            "Epoch 110/5000\n",
            " - 0s - loss: 0.1698 - acc: 0.8045 - val_loss: 0.3879 - val_acc: 0.6531\n",
            "Epoch 111/5000\n",
            " - 0s - loss: 0.1808 - acc: 0.7959 - val_loss: 0.4061 - val_acc: 0.6344\n",
            "Epoch 112/5000\n",
            " - 0s - loss: 0.1736 - acc: 0.8069 - val_loss: 0.3854 - val_acc: 0.6562\n",
            "Epoch 113/5000\n",
            " - 0s - loss: 0.1666 - acc: 0.8170 - val_loss: 0.3814 - val_acc: 0.6375\n",
            "Epoch 114/5000\n",
            " - 0s - loss: 0.1719 - acc: 0.7983 - val_loss: 0.3811 - val_acc: 0.6344\n",
            "Epoch 115/5000\n",
            " - 0s - loss: 0.1651 - acc: 0.8116 - val_loss: 0.3877 - val_acc: 0.6500\n",
            "Epoch 116/5000\n",
            " - 0s - loss: 0.1688 - acc: 0.8092 - val_loss: 0.3772 - val_acc: 0.6344\n",
            "Epoch 117/5000\n",
            " - 0s - loss: 0.1669 - acc: 0.8069 - val_loss: 0.3827 - val_acc: 0.6562\n",
            "Epoch 118/5000\n",
            " - 0s - loss: 0.1654 - acc: 0.8155 - val_loss: 0.3945 - val_acc: 0.6500\n",
            "Epoch 119/5000\n",
            " - 0s - loss: 0.1659 - acc: 0.8147 - val_loss: 0.3913 - val_acc: 0.6531\n",
            "Epoch 120/5000\n",
            " - 0s - loss: 0.1640 - acc: 0.8084 - val_loss: 0.3665 - val_acc: 0.6875\n",
            "Epoch 121/5000\n",
            " - 0s - loss: 0.1700 - acc: 0.7944 - val_loss: 0.3982 - val_acc: 0.6937\n",
            "Epoch 122/5000\n",
            " - 0s - loss: 0.1671 - acc: 0.7975 - val_loss: 0.4040 - val_acc: 0.6562\n",
            "Epoch 123/5000\n",
            " - 0s - loss: 0.1680 - acc: 0.8186 - val_loss: 0.3973 - val_acc: 0.6625\n",
            "Epoch 124/5000\n",
            " - 0s - loss: 0.1638 - acc: 0.8084 - val_loss: 0.4048 - val_acc: 0.6438\n",
            "Epoch 125/5000\n",
            " - 0s - loss: 0.1632 - acc: 0.8124 - val_loss: 0.3839 - val_acc: 0.6844\n",
            "Epoch 126/5000\n",
            " - 0s - loss: 0.1583 - acc: 0.8178 - val_loss: 0.3876 - val_acc: 0.6531\n",
            "Epoch 127/5000\n",
            " - 0s - loss: 0.1622 - acc: 0.8116 - val_loss: 0.3898 - val_acc: 0.6937\n",
            "Epoch 128/5000\n",
            " - 0s - loss: 0.1625 - acc: 0.8124 - val_loss: 0.3878 - val_acc: 0.6500\n",
            "Epoch 129/5000\n",
            " - 0s - loss: 0.1533 - acc: 0.8217 - val_loss: 0.4185 - val_acc: 0.6219\n",
            "Epoch 130/5000\n",
            " - 0s - loss: 0.1639 - acc: 0.8014 - val_loss: 0.4037 - val_acc: 0.6500\n",
            "Epoch 131/5000\n",
            " - 0s - loss: 0.1716 - acc: 0.8006 - val_loss: 0.4068 - val_acc: 0.6687\n",
            "Epoch 132/5000\n",
            " - 0s - loss: 0.1661 - acc: 0.8053 - val_loss: 0.4157 - val_acc: 0.6656\n",
            "Epoch 133/5000\n",
            " - 0s - loss: 0.1592 - acc: 0.8155 - val_loss: 0.4176 - val_acc: 0.6375\n",
            "Epoch 134/5000\n",
            " - 0s - loss: 0.1550 - acc: 0.8225 - val_loss: 0.3759 - val_acc: 0.6750\n",
            "Epoch 135/5000\n",
            " - 0s - loss: 0.1597 - acc: 0.8061 - val_loss: 0.4090 - val_acc: 0.6406\n",
            "Epoch 136/5000\n",
            " - 0s - loss: 0.1545 - acc: 0.8155 - val_loss: 0.3924 - val_acc: 0.6500\n",
            "Epoch 137/5000\n",
            " - 0s - loss: 0.1520 - acc: 0.8264 - val_loss: 0.3957 - val_acc: 0.6406\n",
            "Epoch 138/5000\n",
            " - 0s - loss: 0.1577 - acc: 0.8186 - val_loss: 0.4118 - val_acc: 0.6625\n",
            "Epoch 139/5000\n",
            " - 0s - loss: 0.1552 - acc: 0.8217 - val_loss: 0.4105 - val_acc: 0.6406\n",
            "Epoch 140/5000\n",
            " - 0s - loss: 0.1527 - acc: 0.8210 - val_loss: 0.3968 - val_acc: 0.6562\n",
            "Epoch 141/5000\n",
            " - 0s - loss: 0.1525 - acc: 0.8249 - val_loss: 0.4095 - val_acc: 0.6438\n",
            "Epoch 142/5000\n",
            " - 0s - loss: 0.1511 - acc: 0.8217 - val_loss: 0.4182 - val_acc: 0.6344\n",
            "Epoch 143/5000\n",
            " - 0s - loss: 0.1583 - acc: 0.8194 - val_loss: 0.4073 - val_acc: 0.6531\n",
            "Epoch 144/5000\n",
            " - 0s - loss: 0.1498 - acc: 0.8296 - val_loss: 0.3919 - val_acc: 0.6687\n",
            "Epoch 145/5000\n",
            " - 0s - loss: 0.1447 - acc: 0.8389 - val_loss: 0.3870 - val_acc: 0.6687\n",
            "Epoch 146/5000\n",
            " - 0s - loss: 0.1483 - acc: 0.8210 - val_loss: 0.4008 - val_acc: 0.6594\n",
            "Epoch 147/5000\n",
            " - 0s - loss: 0.1490 - acc: 0.8272 - val_loss: 0.3927 - val_acc: 0.6656\n",
            "Epoch 148/5000\n",
            " - 0s - loss: 0.1472 - acc: 0.8288 - val_loss: 0.3790 - val_acc: 0.6844\n",
            "Epoch 149/5000\n",
            " - 0s - loss: 0.1453 - acc: 0.8342 - val_loss: 0.4152 - val_acc: 0.6687\n",
            "Epoch 150/5000\n",
            " - 0s - loss: 0.1554 - acc: 0.8225 - val_loss: 0.3883 - val_acc: 0.6687\n",
            "Epoch 151/5000\n",
            " - 0s - loss: 0.1470 - acc: 0.8358 - val_loss: 0.3988 - val_acc: 0.6500\n",
            "Epoch 152/5000\n",
            " - 0s - loss: 0.1404 - acc: 0.8405 - val_loss: 0.4112 - val_acc: 0.6375\n",
            "Epoch 153/5000\n",
            " - 0s - loss: 0.1426 - acc: 0.8366 - val_loss: 0.4079 - val_acc: 0.6531\n",
            "Epoch 154/5000\n",
            " - 0s - loss: 0.1388 - acc: 0.8382 - val_loss: 0.4149 - val_acc: 0.6531\n",
            "Epoch 155/5000\n",
            " - 0s - loss: 0.1415 - acc: 0.8428 - val_loss: 0.3925 - val_acc: 0.6531\n",
            "Epoch 156/5000\n",
            " - 0s - loss: 0.1447 - acc: 0.8382 - val_loss: 0.4349 - val_acc: 0.6687\n",
            "Epoch 157/5000\n",
            " - 0s - loss: 0.1546 - acc: 0.8202 - val_loss: 0.4116 - val_acc: 0.6750\n",
            "Epoch 158/5000\n",
            " - 0s - loss: 0.1372 - acc: 0.8350 - val_loss: 0.3949 - val_acc: 0.6500\n",
            "Epoch 159/5000\n",
            " - 0s - loss: 0.1323 - acc: 0.8475 - val_loss: 0.4013 - val_acc: 0.6813\n",
            "Epoch 160/5000\n",
            " - 0s - loss: 0.1481 - acc: 0.8358 - val_loss: 0.3883 - val_acc: 0.6813\n",
            "Epoch 161/5000\n",
            " - 0s - loss: 0.1381 - acc: 0.8475 - val_loss: 0.3971 - val_acc: 0.6844\n",
            "Epoch 162/5000\n",
            " - 0s - loss: 0.1385 - acc: 0.8342 - val_loss: 0.4022 - val_acc: 0.6656\n",
            "Epoch 163/5000\n",
            " - 0s - loss: 0.1300 - acc: 0.8514 - val_loss: 0.3978 - val_acc: 0.6875\n",
            "Epoch 164/5000\n",
            " - 0s - loss: 0.1330 - acc: 0.8491 - val_loss: 0.4099 - val_acc: 0.6687\n",
            "Epoch 165/5000\n",
            " - 0s - loss: 0.1373 - acc: 0.8428 - val_loss: 0.4183 - val_acc: 0.6219\n",
            "Epoch 166/5000\n",
            " - 0s - loss: 0.1430 - acc: 0.8288 - val_loss: 0.4157 - val_acc: 0.6438\n",
            "Epoch 167/5000\n",
            " - 0s - loss: 0.1395 - acc: 0.8382 - val_loss: 0.4002 - val_acc: 0.6750\n",
            "Epoch 168/5000\n",
            " - 0s - loss: 0.1355 - acc: 0.8444 - val_loss: 0.4209 - val_acc: 0.6312\n",
            "Epoch 169/5000\n",
            " - 0s - loss: 0.1443 - acc: 0.8319 - val_loss: 0.4069 - val_acc: 0.6656\n",
            "Epoch 170/5000\n",
            " - 0s - loss: 0.1321 - acc: 0.8452 - val_loss: 0.3926 - val_acc: 0.6594\n",
            "Epoch 171/5000\n",
            " - 0s - loss: 0.1334 - acc: 0.8436 - val_loss: 0.4007 - val_acc: 0.6500\n",
            "Epoch 172/5000\n",
            " - 0s - loss: 0.1343 - acc: 0.8444 - val_loss: 0.4022 - val_acc: 0.6625\n",
            "Epoch 173/5000\n",
            " - 0s - loss: 0.1320 - acc: 0.8491 - val_loss: 0.4029 - val_acc: 0.6750\n",
            "Epoch 174/5000\n",
            " - 0s - loss: 0.1363 - acc: 0.8366 - val_loss: 0.4020 - val_acc: 0.6562\n",
            "Epoch 175/5000\n",
            " - 0s - loss: 0.1309 - acc: 0.8499 - val_loss: 0.4112 - val_acc: 0.6438\n",
            "Epoch 176/5000\n",
            " - 0s - loss: 0.1270 - acc: 0.8499 - val_loss: 0.4326 - val_acc: 0.6406\n",
            "Epoch 177/5000\n",
            " - 0s - loss: 0.1317 - acc: 0.8436 - val_loss: 0.4043 - val_acc: 0.6531\n",
            "Epoch 178/5000\n",
            " - 0s - loss: 0.1331 - acc: 0.8483 - val_loss: 0.4240 - val_acc: 0.6719\n",
            "Epoch 179/5000\n",
            " - 0s - loss: 0.1422 - acc: 0.8335 - val_loss: 0.4101 - val_acc: 0.6500\n",
            "Epoch 180/5000\n",
            " - 0s - loss: 0.1313 - acc: 0.8483 - val_loss: 0.3927 - val_acc: 0.6687\n",
            "Epoch 181/5000\n",
            " - 0s - loss: 0.1301 - acc: 0.8389 - val_loss: 0.4039 - val_acc: 0.6813\n",
            "Epoch 182/5000\n",
            " - 0s - loss: 0.1247 - acc: 0.8577 - val_loss: 0.4160 - val_acc: 0.6750\n",
            "Epoch 183/5000\n",
            " - 0s - loss: 0.1309 - acc: 0.8561 - val_loss: 0.3967 - val_acc: 0.6844\n",
            "Epoch 184/5000\n",
            " - 0s - loss: 0.1342 - acc: 0.8436 - val_loss: 0.4002 - val_acc: 0.6438\n",
            "Epoch 185/5000\n",
            " - 0s - loss: 0.1316 - acc: 0.8382 - val_loss: 0.3875 - val_acc: 0.6594\n",
            "Epoch 186/5000\n",
            " - 0s - loss: 0.1282 - acc: 0.8530 - val_loss: 0.4024 - val_acc: 0.6375\n",
            "Epoch 187/5000\n",
            " - 0s - loss: 0.1289 - acc: 0.8507 - val_loss: 0.4192 - val_acc: 0.6500\n",
            "Epoch 188/5000\n",
            " - 0s - loss: 0.1183 - acc: 0.8679 - val_loss: 0.4182 - val_acc: 0.6656\n",
            "Epoch 189/5000\n",
            " - 0s - loss: 0.1156 - acc: 0.8624 - val_loss: 0.4081 - val_acc: 0.6937\n",
            "Epoch 190/5000\n",
            " - 0s - loss: 0.1181 - acc: 0.8663 - val_loss: 0.4172 - val_acc: 0.6438\n",
            "Epoch 191/5000\n",
            " - 0s - loss: 0.1192 - acc: 0.8640 - val_loss: 0.4001 - val_acc: 0.6656\n",
            "Epoch 192/5000\n",
            " - 0s - loss: 0.1210 - acc: 0.8577 - val_loss: 0.4158 - val_acc: 0.6344\n",
            "Epoch 193/5000\n",
            " - 0s - loss: 0.1213 - acc: 0.8655 - val_loss: 0.4373 - val_acc: 0.6000\n",
            "Epoch 194/5000\n",
            " - 0s - loss: 0.1383 - acc: 0.8452 - val_loss: 0.4554 - val_acc: 0.5938\n",
            "Epoch 195/5000\n",
            " - 0s - loss: 0.1436 - acc: 0.8335 - val_loss: 0.3946 - val_acc: 0.6875\n",
            "Epoch 196/5000\n",
            " - 0s - loss: 0.1140 - acc: 0.8765 - val_loss: 0.4228 - val_acc: 0.6656\n",
            "Epoch 197/5000\n",
            " - 0s - loss: 0.1208 - acc: 0.8569 - val_loss: 0.4362 - val_acc: 0.6750\n",
            "Epoch 198/5000\n",
            " - 0s - loss: 0.1217 - acc: 0.8561 - val_loss: 0.4131 - val_acc: 0.6813\n",
            "Epoch 199/5000\n",
            " - 0s - loss: 0.1171 - acc: 0.8694 - val_loss: 0.3975 - val_acc: 0.6719\n",
            "Epoch 200/5000\n",
            " - 0s - loss: 0.1229 - acc: 0.8577 - val_loss: 0.3991 - val_acc: 0.6594\n",
            "Epoch 201/5000\n",
            " - 0s - loss: 0.1161 - acc: 0.8640 - val_loss: 0.3974 - val_acc: 0.6687\n",
            "Epoch 202/5000\n",
            " - 0s - loss: 0.1136 - acc: 0.8710 - val_loss: 0.4105 - val_acc: 0.6562\n",
            "Epoch 203/5000\n",
            " - 0s - loss: 0.1137 - acc: 0.8679 - val_loss: 0.3994 - val_acc: 0.6562\n",
            "Epoch 204/5000\n",
            " - 0s - loss: 0.1145 - acc: 0.8640 - val_loss: 0.4050 - val_acc: 0.6625\n",
            "Epoch 205/5000\n",
            " - 0s - loss: 0.1128 - acc: 0.8718 - val_loss: 0.4167 - val_acc: 0.6500\n",
            "Epoch 206/5000\n",
            " - 0s - loss: 0.1078 - acc: 0.8804 - val_loss: 0.4079 - val_acc: 0.6781\n",
            "Epoch 207/5000\n",
            " - 0s - loss: 0.1118 - acc: 0.8772 - val_loss: 0.4217 - val_acc: 0.6625\n",
            "Epoch 208/5000\n",
            " - 0s - loss: 0.1083 - acc: 0.8772 - val_loss: 0.4237 - val_acc: 0.6594\n",
            "Epoch 209/5000\n",
            " - 0s - loss: 0.1160 - acc: 0.8640 - val_loss: 0.4415 - val_acc: 0.6344\n",
            "Epoch 210/5000\n",
            " - 0s - loss: 0.1183 - acc: 0.8608 - val_loss: 0.4301 - val_acc: 0.6781\n",
            "Epoch 211/5000\n",
            " - 0s - loss: 0.1070 - acc: 0.8757 - val_loss: 0.4266 - val_acc: 0.6281\n",
            "Epoch 212/5000\n",
            " - 0s - loss: 0.1169 - acc: 0.8663 - val_loss: 0.4138 - val_acc: 0.6719\n",
            "Epoch 213/5000\n",
            " - 0s - loss: 0.1089 - acc: 0.8710 - val_loss: 0.4078 - val_acc: 0.6844\n",
            "Epoch 214/5000\n",
            " - 0s - loss: 0.1115 - acc: 0.8788 - val_loss: 0.4307 - val_acc: 0.6656\n",
            "Epoch 215/5000\n",
            " - 0s - loss: 0.1112 - acc: 0.8780 - val_loss: 0.4151 - val_acc: 0.6594\n",
            "Epoch 216/5000\n",
            " - 0s - loss: 0.1087 - acc: 0.8726 - val_loss: 0.4166 - val_acc: 0.6750\n",
            "Epoch 217/5000\n",
            " - 0s - loss: 0.1089 - acc: 0.8772 - val_loss: 0.4130 - val_acc: 0.6375\n",
            "Epoch 218/5000\n",
            " - 0s - loss: 0.1147 - acc: 0.8624 - val_loss: 0.4213 - val_acc: 0.6438\n",
            "Epoch 219/5000\n",
            " - 0s - loss: 0.1250 - acc: 0.8561 - val_loss: 0.4015 - val_acc: 0.6781\n",
            "Epoch 220/5000\n",
            " - 0s - loss: 0.1160 - acc: 0.8694 - val_loss: 0.4149 - val_acc: 0.6625\n",
            "Epoch 221/5000\n",
            " - 0s - loss: 0.1107 - acc: 0.8780 - val_loss: 0.4408 - val_acc: 0.6344\n",
            "Epoch 222/5000\n",
            " - 0s - loss: 0.1091 - acc: 0.8819 - val_loss: 0.4044 - val_acc: 0.6844\n",
            "Epoch 223/5000\n",
            " - 0s - loss: 0.1025 - acc: 0.8819 - val_loss: 0.4195 - val_acc: 0.6687\n",
            "Epoch 224/5000\n",
            " - 0s - loss: 0.1091 - acc: 0.8765 - val_loss: 0.4095 - val_acc: 0.6625\n",
            "Epoch 225/5000\n",
            " - 0s - loss: 0.1041 - acc: 0.8851 - val_loss: 0.4123 - val_acc: 0.6656\n",
            "Epoch 226/5000\n",
            " - 0s - loss: 0.1029 - acc: 0.8866 - val_loss: 0.4066 - val_acc: 0.6687\n",
            "Epoch 227/5000\n",
            " - 0s - loss: 0.1048 - acc: 0.8804 - val_loss: 0.4297 - val_acc: 0.6656\n",
            "Epoch 228/5000\n",
            " - 0s - loss: 0.1073 - acc: 0.8772 - val_loss: 0.4288 - val_acc: 0.6656\n",
            "Epoch 229/5000\n",
            " - 0s - loss: 0.1005 - acc: 0.8851 - val_loss: 0.4216 - val_acc: 0.6750\n",
            "Epoch 230/5000\n",
            " - 0s - loss: 0.0979 - acc: 0.8890 - val_loss: 0.4215 - val_acc: 0.6594\n",
            "Epoch 231/5000\n",
            " - 0s - loss: 0.0979 - acc: 0.8858 - val_loss: 0.4481 - val_acc: 0.6625\n",
            "Epoch 232/5000\n",
            " - 0s - loss: 0.1017 - acc: 0.8898 - val_loss: 0.4151 - val_acc: 0.6625\n",
            "Epoch 233/5000\n",
            " - 0s - loss: 0.1012 - acc: 0.8858 - val_loss: 0.4187 - val_acc: 0.6500\n",
            "Epoch 234/5000\n",
            " - 0s - loss: 0.0995 - acc: 0.8991 - val_loss: 0.4253 - val_acc: 0.6844\n",
            "Epoch 235/5000\n",
            " - 0s - loss: 0.0989 - acc: 0.8882 - val_loss: 0.4393 - val_acc: 0.6562\n",
            "Epoch 236/5000\n",
            " - 0s - loss: 0.1017 - acc: 0.8858 - val_loss: 0.4329 - val_acc: 0.6687\n",
            "Epoch 237/5000\n",
            " - 0s - loss: 0.0986 - acc: 0.8929 - val_loss: 0.4042 - val_acc: 0.6875\n",
            "Epoch 238/5000\n",
            " - 0s - loss: 0.1057 - acc: 0.8827 - val_loss: 0.4127 - val_acc: 0.6625\n",
            "Epoch 239/5000\n",
            " - 0s - loss: 0.1043 - acc: 0.8835 - val_loss: 0.4262 - val_acc: 0.6625\n",
            "Epoch 240/5000\n",
            " - 0s - loss: 0.1062 - acc: 0.8757 - val_loss: 0.4609 - val_acc: 0.6531\n",
            "Epoch 241/5000\n",
            " - 0s - loss: 0.1193 - acc: 0.8772 - val_loss: 0.4390 - val_acc: 0.6531\n",
            "Epoch 242/5000\n",
            " - 0s - loss: 0.1089 - acc: 0.8733 - val_loss: 0.4117 - val_acc: 0.6719\n",
            "Epoch 243/5000\n",
            " - 0s - loss: 0.1042 - acc: 0.8804 - val_loss: 0.4218 - val_acc: 0.6562\n",
            "Epoch 244/5000\n",
            " - 0s - loss: 0.1037 - acc: 0.8804 - val_loss: 0.4233 - val_acc: 0.6750\n",
            "Epoch 245/5000\n",
            " - 0s - loss: 0.0965 - acc: 0.8890 - val_loss: 0.4181 - val_acc: 0.6750\n",
            "Epoch 246/5000\n",
            " - 0s - loss: 0.0955 - acc: 0.8882 - val_loss: 0.4194 - val_acc: 0.6719\n",
            "Epoch 247/5000\n",
            " - 0s - loss: 0.0967 - acc: 0.8905 - val_loss: 0.4144 - val_acc: 0.6500\n",
            "Epoch 248/5000\n",
            " - 0s - loss: 0.0964 - acc: 0.8913 - val_loss: 0.4339 - val_acc: 0.6875\n",
            "Epoch 249/5000\n",
            " - 0s - loss: 0.1002 - acc: 0.8796 - val_loss: 0.4194 - val_acc: 0.6750\n",
            "Epoch 250/5000\n",
            " - 0s - loss: 0.0970 - acc: 0.8952 - val_loss: 0.4303 - val_acc: 0.6594\n",
            "Epoch 251/5000\n",
            " - 0s - loss: 0.0993 - acc: 0.8827 - val_loss: 0.4537 - val_acc: 0.6500\n",
            "Epoch 252/5000\n",
            " - 0s - loss: 0.1041 - acc: 0.8765 - val_loss: 0.4273 - val_acc: 0.6562\n",
            "Epoch 253/5000\n",
            " - 0s - loss: 0.1011 - acc: 0.8843 - val_loss: 0.4546 - val_acc: 0.6250\n",
            "Epoch 254/5000\n",
            " - 0s - loss: 0.1032 - acc: 0.8812 - val_loss: 0.4272 - val_acc: 0.6750\n",
            "Epoch 255/5000\n",
            " - 0s - loss: 0.1017 - acc: 0.8960 - val_loss: 0.4092 - val_acc: 0.6750\n",
            "Epoch 256/5000\n",
            " - 0s - loss: 0.0952 - acc: 0.8984 - val_loss: 0.4331 - val_acc: 0.6687\n",
            "Epoch 257/5000\n",
            " - 0s - loss: 0.0935 - acc: 0.8905 - val_loss: 0.4146 - val_acc: 0.6656\n",
            "Epoch 258/5000\n",
            " - 0s - loss: 0.1076 - acc: 0.8749 - val_loss: 0.4320 - val_acc: 0.6500\n",
            "Epoch 259/5000\n",
            " - 0s - loss: 0.0924 - acc: 0.8960 - val_loss: 0.4269 - val_acc: 0.6625\n",
            "Epoch 260/5000\n",
            " - 0s - loss: 0.0893 - acc: 0.9007 - val_loss: 0.4282 - val_acc: 0.6594\n",
            "Epoch 261/5000\n",
            " - 0s - loss: 0.0898 - acc: 0.8944 - val_loss: 0.4228 - val_acc: 0.6719\n",
            "Epoch 262/5000\n",
            " - 0s - loss: 0.0897 - acc: 0.8960 - val_loss: 0.4434 - val_acc: 0.6656\n",
            "Epoch 263/5000\n",
            " - 0s - loss: 0.0953 - acc: 0.8968 - val_loss: 0.4558 - val_acc: 0.6625\n",
            "Epoch 264/5000\n",
            " - 0s - loss: 0.0988 - acc: 0.8851 - val_loss: 0.4423 - val_acc: 0.6469\n",
            "Epoch 265/5000\n",
            " - 0s - loss: 0.1093 - acc: 0.8812 - val_loss: 0.4412 - val_acc: 0.6281\n",
            "Epoch 266/5000\n",
            " - 0s - loss: 0.1042 - acc: 0.8796 - val_loss: 0.4419 - val_acc: 0.6562\n",
            "Epoch 267/5000\n",
            " - 0s - loss: 0.0958 - acc: 0.8929 - val_loss: 0.4297 - val_acc: 0.6719\n",
            "Epoch 268/5000\n",
            " - 0s - loss: 0.0896 - acc: 0.9007 - val_loss: 0.4167 - val_acc: 0.6625\n",
            "Epoch 269/5000\n",
            " - 0s - loss: 0.0878 - acc: 0.9046 - val_loss: 0.4505 - val_acc: 0.6438\n",
            "Epoch 270/5000\n",
            " - 0s - loss: 0.0908 - acc: 0.8984 - val_loss: 0.4227 - val_acc: 0.6625\n",
            "Epoch 271/5000\n",
            " - 0s - loss: 0.0883 - acc: 0.9077 - val_loss: 0.4314 - val_acc: 0.6562\n",
            "Epoch 272/5000\n",
            " - 0s - loss: 0.0931 - acc: 0.9030 - val_loss: 0.4212 - val_acc: 0.6562\n",
            "Epoch 273/5000\n",
            " - 0s - loss: 0.0913 - acc: 0.8968 - val_loss: 0.4377 - val_acc: 0.6625\n",
            "Epoch 274/5000\n",
            " - 0s - loss: 0.0944 - acc: 0.8960 - val_loss: 0.4543 - val_acc: 0.6594\n",
            "Epoch 275/5000\n",
            " - 0s - loss: 0.0877 - acc: 0.9007 - val_loss: 0.4279 - val_acc: 0.6687\n",
            "Epoch 276/5000\n",
            " - 0s - loss: 0.0886 - acc: 0.8999 - val_loss: 0.4487 - val_acc: 0.6687\n",
            "Epoch 277/5000\n",
            " - 0s - loss: 0.0869 - acc: 0.9023 - val_loss: 0.4217 - val_acc: 0.6625\n",
            "Epoch 278/5000\n",
            " - 0s - loss: 0.0839 - acc: 0.9101 - val_loss: 0.4196 - val_acc: 0.6687\n",
            "Epoch 279/5000\n",
            " - 0s - loss: 0.0888 - acc: 0.9070 - val_loss: 0.4350 - val_acc: 0.6750\n",
            "Epoch 280/5000\n",
            " - 0s - loss: 0.0903 - acc: 0.8874 - val_loss: 0.4255 - val_acc: 0.6469\n",
            "Epoch 281/5000\n",
            " - 0s - loss: 0.0905 - acc: 0.9046 - val_loss: 0.4087 - val_acc: 0.6781\n",
            "Epoch 282/5000\n",
            " - 0s - loss: 0.0897 - acc: 0.9023 - val_loss: 0.4216 - val_acc: 0.6656\n",
            "Epoch 283/5000\n",
            " - 0s - loss: 0.0875 - acc: 0.9093 - val_loss: 0.4332 - val_acc: 0.6656\n",
            "Epoch 284/5000\n",
            " - 0s - loss: 0.0826 - acc: 0.9101 - val_loss: 0.4485 - val_acc: 0.6687\n",
            "Epoch 285/5000\n",
            " - 0s - loss: 0.0777 - acc: 0.9187 - val_loss: 0.4595 - val_acc: 0.6813\n",
            "Epoch 286/5000\n",
            " - 0s - loss: 0.0848 - acc: 0.9093 - val_loss: 0.4386 - val_acc: 0.6531\n",
            "Epoch 287/5000\n",
            " - 0s - loss: 0.0830 - acc: 0.9038 - val_loss: 0.4276 - val_acc: 0.6594\n",
            "Epoch 288/5000\n",
            " - 0s - loss: 0.0856 - acc: 0.9101 - val_loss: 0.4300 - val_acc: 0.6656\n",
            "Epoch 289/5000\n",
            " - 0s - loss: 0.0844 - acc: 0.9124 - val_loss: 0.4214 - val_acc: 0.6531\n",
            "Epoch 290/5000\n",
            " - 0s - loss: 0.0837 - acc: 0.9148 - val_loss: 0.4418 - val_acc: 0.6594\n",
            "Epoch 291/5000\n",
            " - 0s - loss: 0.0803 - acc: 0.9093 - val_loss: 0.4516 - val_acc: 0.6625\n",
            "Epoch 292/5000\n",
            " - 0s - loss: 0.0788 - acc: 0.9163 - val_loss: 0.4324 - val_acc: 0.6813\n",
            "Epoch 293/5000\n",
            " - 0s - loss: 0.0794 - acc: 0.9124 - val_loss: 0.4377 - val_acc: 0.6562\n",
            "Epoch 294/5000\n",
            " - 0s - loss: 0.0777 - acc: 0.9140 - val_loss: 0.4193 - val_acc: 0.6750\n",
            "Epoch 295/5000\n",
            " - 0s - loss: 0.0759 - acc: 0.9210 - val_loss: 0.4394 - val_acc: 0.6562\n",
            "Epoch 296/5000\n",
            " - 0s - loss: 0.0845 - acc: 0.9007 - val_loss: 0.4524 - val_acc: 0.6687\n",
            "Epoch 297/5000\n",
            " - 0s - loss: 0.0866 - acc: 0.9062 - val_loss: 0.4330 - val_acc: 0.6687\n",
            "Epoch 298/5000\n",
            " - 0s - loss: 0.0829 - acc: 0.9085 - val_loss: 0.4246 - val_acc: 0.6625\n",
            "Epoch 299/5000\n",
            " - 0s - loss: 0.0832 - acc: 0.9077 - val_loss: 0.4476 - val_acc: 0.6781\n",
            "Epoch 00299: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8923f74518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E86ixomDM8Kq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bc0b460-1667-45f4-e23e-d868acf6e9d2"
      },
      "source": [
        "best_model = model\n",
        "best_model.load_weights('gdrive/My Drive/DeepLearningCodeBook/multi_layer_best_model.h5')\n",
        "best_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Evaluate on test set\n",
        "score = best_model.evaluate(X_test.values, y_test, verbose=0)\n",
        "print('Test accuracy: %.2f%%' % (score[1]*100))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 70.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JCqUxvRM8PT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuU_wtYQM8Jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxLY6reQM8En",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}